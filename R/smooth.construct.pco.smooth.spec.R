#' Build a PCoRR smoother
#'
#' Build a basis and (ridge) penalty that allows one to regress the response on leading principal coordinates defined by a relevant distance among the functional predictors.
#'
#'
#' @aliases pco smooth.construct.pco.smooth.spec Predict.matrix.pco.smooth
#' @export
#' @import mgcv
#'
#' @param object a smooth specification object, usually generated by a term \code{s(..., bs="pco", ...)}. Note that \code{xt} object is required, see Details, below.
#' @param data a list containing just the data.
#' @param knots IGNORED!
#'
#' @return An object of class \code{pco.smooth}. The resulting object has an \code{xt} element which contains details of the multidimensional scaling, which may be interesting.
#'
#' @section Details:
#' The constructor is not normally called directly, but is rather used internally by \code{\link{gam}}. To use for basis setup it is recommended to use \code{\link{smooth.construct2}}.
#'
#' The basis dimension \code{k} will be used as the truncation for the multidimensional scaling (so \code{k=9} will give a 9-dimensional MDS projection).
#'
#' When specifying the model extra arguments must be supplied by the \code{xt} argument. Two forms are possible:
#'
#' First supplying:
#' \itemize{
#'   \item \code{realdata} the actual data to use for the model, usual args are ignored\cr
#'   \item \code{dist_fn} distance function. It will take one argument: a \code{data.frame} or \code{matrix} of data locations and returns a square distance matrix with number of rows and columns equal to the number of rows in the data passed to it.
#' }
#' These will create a distance matrix to be projected.
#'
#' Instead one may supply a distance matrix directly \code{D=...} .
#'
#' \code{xt} also has the following options:
#' \itemize{
#'   \item \code{add} if \code{TRUE} then a constant is added to the distance matrix before the MDS is calculated, ensuring that the distances are non-negative. See \code{\link{cmdscale}} for details. (Default \code{FALSE}.)\cr
#'    \item \code{fastcmd} should eigendecompositions of the distance matrix be calculated using \code{\link{slanczos}} (\code{fastcmd=TRUE}) or using \code{\link{cmdscale}} (\code{fastcmd=FALSE})? In the former case eigendecompositions used for the multidimensional scaling will use a Lanczos iteration to calculate the first \code{k} eigenvalues/vectors quickly.
#' }
#'
#' @author David L Miller, based on code from Lan Huo and Phil Reiss
#'
#' @examples
#' # a simulated example
#'
#' require(poridge)
#' require(dtw)
#'
#' ## First generate the data
#' Xnl <- matrix(0, 30, 101)
#' set.seed(813)
#' tt <- sort(sample(1:90, 30))
#' for(i in 1:30){
#'   Xnl[i, tt[i]:(tt[i]+4)] <- -1
#'   Xnl[i, (tt[i]+5):(tt[i]+9)] <- 1
#' }
#' X.toy <- Xnl + matrix(rnorm(30*101, ,0.05), 30)
#' y.toy <- tt + rnorm(30, 0.05)
#' y.rainbow <- rainbow(30, end=0.9)[(y.toy-min(y.toy))/
#'                                    diff(range(y.toy))*29+1]
#'
#' ## Display the toy data
#' par(mfrow=c(2, 2))
#' matplot((0:100)/100, t(Xnl[c(4, 25), ]), type="l", xlab="t", ylab="",
#'         ylim=range(X.toy), main="Noiseless functions")
#' matplot((0:100)/100, t(X.toy[c(4, 25), ]), type="l", xlab="t", ylab="",
#'         ylim=range(X.toy), main="Observed functions")
#' matplot((0:100)/100, t(X.toy), type="l", lty=1, col=y.rainbow, xlab="t",
#'         ylab="", main="Rainbow plot")
#'
#' ## Obtain DTW distances
#' D <- dist(X.toy, method="dtw", window.type="sakoechiba", window.size=5)
#'
#' ## Compare PC vs. PCo ridge regression
#'
#' # matrix to store results
#' GCVmat <- matrix(NA, 15, 2)
#' # dummy response variable
#' dummy <- rep(1,30)
#'
#' # loop over possible projection dimensions
#' for(k. in 1:15){
#'   # fit models
#'   m1 <- gam(y.toy ~ s(dummy, bs="pco", k=k.,
#'             xt=list(realdata=X.toy, dist_fn=dist)), method="REML")
#'   m2 <- gam(y.toy ~ s(dummy, bs="pco", k=k., xt=list(D=D)), method="REML")
#'   # calculate and store GCV scores
#'   GCVmat[k., ] <- length(y.toy) * c(sum(m1$residuals^2)/m1$df.residual^2,
#'                    sum(m2$residuals^2)/m2$df.residual^2)
#' }
#'
#' ## plot the GCV scores per dimension for each model
#' matplot(GCVmat, lty=1:2, col=1, pch=16:17, type="o", ylab="GCV",
#'         xlab="Number of principal components / coordinates",
#'         main="GCV score")
#' legend("right", c("PCR", "DTW-based PCoR"), lty=1:2, pch=16:17)
smooth.construct.pco.smooth.spec <- function(object, data, knots){

  ## test what we got given
  # all the extra stuff gets put in object$xt
  xt <- object$xt
  if(all(c("D","realdata","dist_fn") %in% names(xt)) |
     all(c("D","dist_fn") %in% names(xt)) |
     all(c("D","realdata") %in% names(xt)) |
     !any(c("D", "realdata", "dist_fn") %in% names(xt))){
    stop("Please supply either a distance matrix or data and distance function!")
  }

  # distance matrix
  if(all(c("realdata","dist_fn") %in% names(xt))){
    D <- xt$dist_fn(xt$realdata)
  }else{
    D <- xt$D
  }

  # projection dimension
  pdim <- object$bs.dim

  ## do some input checking
  # either K or D must be supplied
  if(is.null(D)) {
    stop("No distance matrix!")
  }
  # default to use regular cmdscale
  if(is.null(xt$fastcmd)){
    xt$fastcmd <- FALSE
  }

  ## if K not supplied then compute from D?
  #if(is.null(K) & !is.null(D)) {
  #  D <- d2k(D, cailliez = cailliez, truncate = truncate)
  #}


  # use the additive constant?
  # default to FALSE
  if(!is.null(xt$add)){
    add <- xt$add
  }else{
    add <- FALSE
  }

  # what do cmdscale options mean?!
  # k     - dimension of MDS projection
  # eig   - return eigenvalues
  # x.ret - return (double centred distance matrix)
  # add   - add a constant so D is Euclidean (cov() gives -ve
  #         values, which is not a property of a distance.
  if(xt$fastcmd){
    # use lanczos for the eigendecomposition
    mds.obj <- cmdscale_lanczos(D, k=pdim, eig=TRUE, x.ret=TRUE, add=add)
  }else{
    mds.obj <- cmdscale(D, k=pdim, eig=TRUE, x.ret=TRUE, add=add)
  }

  if(sum(mds.obj$eig>0) < pdim){
    stop("Only the first ",sum(mds.obj$eig>0)," eigenvalues are positive, this is the maximum projection dimension without setting 'add=TRUE', see ?smooth.construct.pco.smooth.spec for further information.")
  }

  ## four required additions to the return object:
  # model matrix
  object$X <- mds.obj$points
  colnames(object$X) <- paste0("pco_", 1:ncol(object$X))
  # penalty matrix
  object$S <- list(diag(nrow = pdim))
  # penalty rank
  object$rank <- array(pdim, dim=c(1,1))
  # null space dimension
  object$null.space.dim <- 0

  # set the label (for plots and summary)
  object$label <- object$term

  # now reset the terms so we include ALL possible
  # variables (though we limit to pdim in reality)
  if(all(c("realdata", "dist_fn") %in% names(xt))){
    object$term <- paste0("pco_", 1:ncol(object$xt$realdata))
  }else{
    object$term <- paste0("pco_", 1)
  }

  # store dimension
  object$dim <- pdim

  ## extra options
  # don't allow tensor products
  object$te.ok <- 0
  # probably best not to plot this with plot.gam
  # (what would the x axis be?)
  object$plot.me <- FALSE

  # see ?smooth.construct for what these mean!
  object$C <- array(dim=c(0, 1))
  object$side.constrain <- FALSE

  object$no.rescale <- TRUE

  # save mds object returned by cmdscale
  object$xt$mds.obj <- mds.obj

  # give it some class
  class(object) <- "pco.smooth"
  return(object)
}
